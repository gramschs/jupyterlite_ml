{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a838374a",
   "metadata": {},
   "source": [
    "# 10.2 Training SVM mit Scikit-Learn\n",
    "\n",
    "Wenn wir in der Dokumentation von Scikit-Learn\n",
    "[Scikit-Learn/SVM](https://scikit-learn.org/stable/modules/svm.html) die Support\n",
    "Vector Machines nachschlagen, so finden wir viele Einträge: SVC, NuSVC,\n",
    "LinearSVC, SVR, NuSVR und LinearSVR. Die Varianten mit \"C\" stehen für\n",
    "Klassifikation (englisch Classification) und die Varianten mit \"R\" für\n",
    "Regression. Wir benutzen in diesem Kapitel das Modell SVC.\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "* Sie können ein SVM-Modell mit der Klasse SVC erzeugen.\n",
    "* Sie können den Parameter C zur Steuerung der Margins einsetzen.\n",
    "\n",
    "## Wahl des linearen Kernels\n",
    "\n",
    "Zuerst importieren wir aus Scikit-Learn das entsprechende Modul 'SVM' und\n",
    "instantiieren ein Modell. Da wir die etwas allgemeinere Klasse SVC anstatt\n",
    "LinearSVC verwenden, müssen wir bereits bei der Erzeugung die Option `kernel=`\n",
    "auf linear setzen, also `kernel='linear'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95bfd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_modell = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03ec7b",
   "metadata": {},
   "source": [
    "Wir erzeugen uns erneut künstliche Messdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3bc728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pylab as plt; plt.style.use('bmh')\n",
    "\n",
    "# Erzeugung künstlicher Daten\n",
    "X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.50)\n",
    "\n",
    "# Visualisierung künstlicher Daten\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x = X[:,0], y = X[:,1],  color=y, color_continuous_scale=['#3b4cc0', '#b40426'],\n",
    "                 title='Künstliche Daten',\n",
    "                 labels={'x': 'Feature 1', 'y': 'Feature 2'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4926b0",
   "metadata": {},
   "source": [
    "**Warnung:**\n",
    "SVMs sind empfindlich gegenüber unterschiedlichen Feature-Skalen. Ein Feature mit\n",
    "Werten von 0-1000 dominiert ein Feature mit Werten 0-1, auch wenn beide gleich\n",
    "wichtig sind. Daher sollten Features vor dem Training skaliert werden.\n",
    "\n",
    "Eine Skalierung der Daten ist hier nicht erforderlich, da alle Features im\n",
    "gleichen Wertebereich liegen. Als nächstes teilen wir die Messdaten in\n",
    "Trainings- und Testdaten auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ec4128",
   "metadata": {},
   "source": [
    "Nun können wir unser SVM-Modell trainieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_modell.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46da70",
   "metadata": {},
   "source": [
    "Und als nächstes analysieren, wie viele der Testdaten mit dem trainierten Modell\n",
    "korrekt klassifiziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7954508",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_modell.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea2715",
   "metadata": {},
   "source": [
    "Ein super Ergebnis! Schön wäre es jetzt noch, die gefundene Trenngerade zu\n",
    "visualisieren. Dazu modifizieren wir ein Code-Beispiel aus dem Buch: »Data\n",
    "Science mit Python« von Jake VanderPlas (mitp Verlag 2017), ISBN 978-3-95845-\n",
    "695-2, siehe\n",
    "[https://github.com/jakevdp/PythonDataScienceHandbook](https://github.com/jakevdp/PythonDataScienceHandbook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelle: VanderPlas \"Data Science mit Python\", S. 482\n",
    "# modified by Simone Gramsch\n",
    "import numpy as np\n",
    "\n",
    "def plot_svc_grenze(model):\n",
    "    # aktuelles Grafik-Fenster auswerten\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # Raster für die Auswertung erstellen\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "    # Abstand zur Trennhyperebene berechnen mit eingebauter \n",
    "    # decision_function()\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "\n",
    "    # Entscheidungsgrenzen und Margin darstellen\n",
    "    ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "    # Stützvektoren darstellen\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, linewidth=1, facecolors='none', edgecolors='orange');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74521b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('SVM mit Soft Margin');\n",
    "\n",
    "plot_svc_grenze(svm_modell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392020b7",
   "metadata": {},
   "source": [
    "## Der Parameter C\n",
    "\n",
    "Im letzten Abschnitt haben wir uns mit dem Parameter `C` beschäftigt, der\n",
    "Ausnahmen innerhalb des Sicherheitsstreifens erlaubt. Wir können uns den\n",
    "Parameter `C` als die \"Höhe der Mauer\" um den Margin vorstellen. Eine hohe Mauer\n",
    "(großes `C`) schützt den Sicherheitsbereich streng. Praktisch keine Datenpunkte\n",
    "dürfen hinein. Eine niedrige Mauer (kleines `C`) ist toleranter und lässt mehr\n",
    "Ausnahmen zu. Als nächstes schauen wir uns an, wie der Parameter `C` gesetzt\n",
    "wird.  \n",
    "\n",
    "Die Option zum Setzen des Parameters C lautet schlicht und einfach `C=`. Dabei\n",
    "muss C immer positiv sein. Der voreingestellte Standardwert ist `C=1`.\n",
    "\n",
    "Damit aber besser sichtbar wird, wie sich C auswirkt, vermischen wir die\n",
    "künstlichen Daten stärker. Für die Bewertung der Modelle trennen wir die Daten\n",
    "in Trainings- und Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3367a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugung künstlicher Daten\n",
    "X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.80)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Visualisierung künstlicher Daten\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x = X[:,0], y = X[:,1],  color=y, color_continuous_scale=['#3b4cc0', '#b40426'],\n",
    "                 title='Künstliche Daten',\n",
    "                 labels={'x': 'Feature 1', 'y': 'Feature 2'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ba8d62",
   "metadata": {},
   "source": [
    "Zuerst wählen wir ein sehr hohes `C=1000000`. Das ist ein Hard Margin, eine\n",
    "praktisch nicht durchdringbare Mauer, die keine Datenpunkte in den\n",
    "Sicherheitsbereich lässt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a506d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wahl des Modells mit linearem Kern und großem C\n",
    "svm_modell = svm.SVC(kernel='linear', C=1000000)\n",
    "\n",
    "# Training und Bewertung\n",
    "svm_modell.fit(X_train, y_train);\n",
    "score = svm_modell.score(X_test, y_test)\n",
    "print(f'Score auf den Testdaten: {score:.2f}')\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('SVM mit Hard Margin');\n",
    "plot_svc_grenze(svm_modell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ae6d5",
   "metadata": {},
   "source": [
    "Nun reduzieren wir den Wert des Parameters `C` deutlich auf `C=1`. Vereinzelt\n",
    "liegen Datenpunkte nun im Sicherheitsbereich, d.h. wir haben einen Soft Margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98540ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wahl des Modells mit linearem Kern und kleinem C\n",
    "svm_modell = svm.SVC(kernel='linear', C=1)\n",
    "\n",
    "# Training und Bewertung\n",
    "svm_modell.fit(X_train, y_train);\n",
    "score = svm_modell.score(X_test, y_test)\n",
    "print(f'Score auf den Testdaten: {score:.2f}')\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('SVM mit Soft Margin');\n",
    "plot_svc_grenze(svm_modell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a86dd",
   "metadata": {},
   "source": [
    "Die Visualisierung zeigt, dass bei kleinem `C` mehr Stützvektoren (orange\n",
    "eingekreist) vorhanden sind, und einige davon innerhalb des Margins liegen oder\n",
    "die Klassifikationsgrenze verletzen.\n",
    "\n",
    "Welches `C` sollen wir wählen? Beide Modelle liefern den gleichen Score von 1.0\n",
    "auf den Testdaten. Das kleinere `C` führt jedoch zu einem robusteren Modell mit\n",
    "breiterem Margin. In der Praxis wird `C` oft durch Kreuzvalidierung\n",
    "(Crossvalidation) optimiert, was wir in einem späteren Kapitel aufgreifen\n",
    "werden.\n",
    "\n",
    "## Zusammenfassung\n",
    "\n",
    "Verwenden wir den SVC-Klassifikator aus dem Modul SVM von Scikit-Learn, können\n",
    "wir mittels der Option `kernel='linear'` eine binäre Klassifikation durchführen,\n",
    "bei der die Trennungsgerade den größtmöglichen Abstand zwischen den Gruppen von\n",
    "Punkten erzeugt, also einen möglichst großen Margin. Sind die Daten nicht linear\n",
    "trennbar, so können wir mit der Option `C=` steuern, wie viele Ausnahmen erlaubt\n",
    "werden sollen. Mit Ausnahmen sind Punkte innerhalb des Margins gemeint. Im\n",
    "nächsten Abschnitt betrachten wir nichtlineare Trennungsgrenzen."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
