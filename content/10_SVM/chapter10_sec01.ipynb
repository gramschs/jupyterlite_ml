{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18a5f56",
   "metadata": {},
   "source": [
    "# 10.1 Maximiere den Rand, aber soft\n",
    "\n",
    "Wir bleiben weiter bei den klassischen ML-Methoden und beschäftigen uns in\n",
    "diesem Kapitel mit den Support Vector Machines. Zunächst jedoch ergründen wir\n",
    "das Konzept, das hinter den Support Vector Machines steht.\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "* Sie kennen die Abkürzung **SVM** für **Support Vector Machines**.\n",
    "* Sie kennen die Idee, bei Support Vector Machines den **Margin** (=\n",
    "  Randabstand) zu maximieren.\n",
    "* Sie wissen, was Stützvektoren bzw. **Support Vectors** sind.\n",
    "* Sie wissen, dass ein harter Randabstand nur bei linear trennbaren Datensätzen\n",
    "  möglich ist.\n",
    "* Sie wissen, dass eigentlich nicht trennbare Datensätze mit der Technik **Soft\n",
    "  Margin** (= weicher Randabstand) dennoch klassifiziert werden können.\n",
    "\n",
    "## Welche Trenngerade soll es sein?\n",
    "\n",
    "Support Vector Machines (SVM) können sowohl für Klassifikations- als auch\n",
    "Regressionsprobleme genutzt werden. Insbesondere wenn viele Merkmale (Features)\n",
    "vorliegen, sind SVMs gut geeignet. Auch neigen SVMs weniger zu Overfitting.\n",
    "Daher lohnt es sich, Support Vector Machines anzusehen.\n",
    "\n",
    "Warum SVMs weniger zu Overfitting neigen und mit Ausreißern besser umgehen\n",
    "können, sehen wir bereits an der zugrundeliegenden Idee, die hinter dem\n",
    "Verfahren steckt. Um das Basis-Konzept der SVMs zu erläutern, erzeugen wir\n",
    "künstliche Messdaten. Dazu verwenden wir die Funktion `make_blobs` aus dem\n",
    "Scikit-Learn-Modul `Datasets`. Weitere Details zum Aufruf der Funktion finden\n",
    "wir in der\n",
    "[Scikit-Learn-Dokumentation/make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html?highlight=make+blobs#sklearn.datasets.make_blobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# generate artificial data\n",
    "X, y = make_blobs(n_samples=60, centers=2, random_state=0, cluster_std=0.50)\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77755ea4",
   "metadata": {},
   "source": [
    "Die Funktion `make_blobs` erzeugt standardmäßig zwei Input-Features, da die\n",
    "Option `n_features` auf den Wert 2 voreingestellt ist, und einen Output, bei dem\n",
    "die Labels entweder durch 0 oder 1 gekennzeichnet sind. Durch die Option\n",
    "`random_state=0` wird der Zufall ausgeschaltet.\n",
    "\n",
    "Wenn wir die Daten in ein Pandas-DataFrame packen und anschließend\n",
    "visualisieren, erhalten wir folgenden Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd01594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import plotly.express as px\n",
    "\n",
    "daten = pd.DataFrame({\n",
    "    'Feature 1': X[:,0],\n",
    "    'Feature 2': X[:,1],\n",
    "    'Status': y.astype(bool),\n",
    "    })\n",
    "\n",
    "fig = px.scatter(daten, x = 'Feature 1', y = 'Feature 2',  color='Status',\n",
    "                 title='Künstliche Daten', color_discrete_sequence=['#b40426','#3b4cc0'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccbb671",
   "metadata": {},
   "source": [
    "Wir können uns jetzt verschiedene Geraden vorstellen, die die blauen Punkte von\n",
    "den roten Punkten trennen. In der folgenden Grafik sind drei eingezeichnet.\n",
    "Welche würden Sie nehmen und warum?\n",
    "\n",
    "[https://gramschs.github.io/book_ml4ing/_images/fig10_01_annotated.pdf](https://gramschs.github.io/book_ml4ing/_images/fig10_01_annotated.pdf)\n",
    "\n",
    "Alle drei Geraden trennen die blauen von den roten Punkten. Jedoch könnte Gerade\n",
    "3 problematisch werden, wenn beispielsweise ein neuer blauer Datenpunkt an der\n",
    "Position (2.3, 3.3) dazukäme. Dann würde Gerade 3 diesen Punkt als rot\n",
    "klassifizieren. Ähnlich verhält es sich mit Gerade 1. Ein neuer blauer\n",
    "Datenpunkt an der Position (0.5, 3) würde fälschlicherweise als rot\n",
    "klassifiziert werden. Gerade 2 bietet den sichersten Abstand zu den bereits\n",
    "vorhandenen Datenpunkten. Wir können diesen \"Sicherheitsstreifen\" folgendermaßen\n",
    "visualisieren.\n",
    "\n",
    "[https://gramschs.github.io/book_ml4ing/_images/fig10_02_annotated.pdf](https://gramschs.github.io/book_ml4ing/_images/fig10_02_annotated.pdf)\n",
    "\n",
    "Der Support-Vector-Algorithmus sucht nun die Gerade, die die Datenpunkte mit dem\n",
    "größten Randabstand (= Margin) voneinander trennt. Im Englischen sprechen wir\n",
    "daher auch von **Large Margin Classification**. Die Suche nach dieser Geraden\n",
    "ist dabei etwas zeitaufwändiger als die Berechnung der Gewichte bei der\n",
    "logistischen Regression. Wenn aber einmal das Modell trainiert ist, ist die\n",
    "Prognose effizienter, da nur die sogenannten **Stützvektoren**, auf englisch\n",
    "**Support Vectors** gespeichert und ausgewertet werden. Die Stützvektoren sind\n",
    "die Vektoren, die vom Ursprung des Koordinatensystems zu den Punkten zeigen, die\n",
    "auf der Grenze des Sicherheitsbereichs liegen.  \n",
    "\n",
    "[https://gramschs.github.io/book_ml4ing/_images/fig10_03.pdf](https://gramschs.github.io/book_ml4ing/_images/fig10_03.pdf)\n",
    "\n",
    "## Großer, aber weicher Randabstand\n",
    "\n",
    "Die bisherigen Beispiele zeigten perfekt trennbare Daten. In der Praxis sind\n",
    "Datensätze jedoch oft nicht linear trennbar. Für den Fall, dass einige wenige\n",
    "Datenpunkte \"falsch\" liegen, erlauben wir Ausnahmen. Wie viele Ausnahmen wir\n",
    "erlauben wollen, die im Sicherheitsstreifen liegen, steuern wir mit dem\n",
    "Parameter `C`. Ein großes `C` bedeutet, dass wir eine große Mauer an den Grenzen\n",
    "des Sicherheitsabstandes errichten. Es kommt kaum vor, dass Datenpunkte\n",
    "innerhalb des Margins liegen. Je kleiner `C` wird, desto mehr Datenpunkte sind\n",
    "innerhalb des Sicherheitsbereichs erlaubt.\n",
    "\n",
    "Im Folgenden betrachten wir einen neuen künstlichen Datensatz, bei dem die\n",
    "blauen von den roten Punkte nicht mehr ganz so stark getrennt sind. Schauen Sie\n",
    "sich die fünf verschiedenen Margins an, die entstehen, wenn der Parameter `C`\n",
    "variiert wird.\n",
    "\n",
    "Öffnen Sie\n",
    "\n",
    "[https://gramschs.github.io/book_ml4ing/chapter10/chapter10_sec01.html](https://gramschs.github.io/book_ml4ing/chapter10/chapter10_sec01.html)\n",
    "\n",
    "und scrollen Sie nach ganz unten.\n",
    "\n",
    "## Zusammenfassung\n",
    "\n",
    "In diesem Abschnitt haben wir die Ideen kennengelernt, die den Support Vector\n",
    "Machines zugrunde liegen. Im nächsten Abschnitt schauen wir uns an, wie ein\n",
    "SVM-Modell mit Scikit-Learn trainiert wird."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
