{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e62ef0",
   "metadata": {},
   "source": [
    "# 10.3 Nichtlineare SVM\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "* Sie kennen den **Kernel-Trick**.\n",
    "* Sie können mit den **radialen Basisfunktionen** als neue Option für\n",
    "  SVM-Verfahren nichtlinear trennbare Daten klassifizieren.\n",
    "\n",
    "## Nichtlineare trennbare Daten\n",
    "\n",
    "Für die Support Vector Machines sind wir bisher davon ausgegangen, dass die\n",
    "Daten -- ggf. bis auf wenige Ausnahmen -- linear getrennt werden können. Im\n",
    "Folgenden betrachten wir nun einen künstlichen Messdatensatz, bei dem das\n",
    "offensichtlich nicht geht. Dazu nutzen wir die in Scikit-Learn integrierte\n",
    "Funktion `make_circles()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# künstliche Messdaten generieren\n",
    "X,y = make_circles(100, random_state=0, factor=0.3, noise=0.1)\n",
    "\n",
    "# künstliche Messdaten visualisieren\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x = X[:,0], y = X[:,1],  color=y, color_continuous_scale=['#3b4cc0', '#b40426'],\n",
    "                 title='Künstliche Daten',\n",
    "                 labels={'x': 'Feature 1', 'y': 'Feature 2'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e78c1",
   "metadata": {},
   "source": [
    "Das menschliche Auge erkennt sofort das Muster in den Daten. Ganz offensichtlich\n",
    "sind die roten und blauen Punkte kreisförmig angeordnet und können\n",
    "dementsprechend auch durch einen Kreis getrennt werden. Allerdings wird ein\n",
    "SVM-Klassifikator, so wie wir das SVM-Verfahren bisher kennengelernt haben,\n",
    "versagen. Eine Gerade zur Klassifikation der roten und blauen Punkte passt\n",
    "einfach nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8defa68c",
   "metadata": {},
   "source": [
    "## Aus 2 mach 3\n",
    "\n",
    "Die Idee zur Überwindung dieses Problems klingt zunächst einmal absurd. Wir\n",
    "machen aus zwei Features drei Features. Die dahinterliegende Idee ist: Was in 2D\n",
    "nicht linear trennbar ist, kann in 3D linear trennbar werden. Ähnlich wie ein\n",
    "Schatten (2D) zweier sich berührender Kugeln (3D) sich zu überlappen scheint,\n",
    "obwohl die Kugeln getrennt sind.\n",
    "\n",
    "Als drittes Feature wählen wir den Abstand eines Punktes zum Ursprung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb66a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Extraktion der Daten, damit leichter darauf zugegriffen werden kann\n",
    "X1 = X[:,0]\n",
    "X2 = X[:,1]\n",
    "\n",
    "# neues Feature: Abstand eines Punktes zum Ursprung\n",
    "X3 = np.sqrt(X1**2 + X2**2)\n",
    "\n",
    "fig = px.scatter_3d(x=X1, y=X2, z=X3, color=y, \n",
    "                    color_continuous_scale=['#3b4cc0', '#b40426'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08496f13",
   "metadata": {},
   "source": [
    "Bitte drehen Sie die interaktive 3D-Ansicht mit der Maus (klicken und ziehen),\n",
    "bis die z-Achse nach oben zeigt. Die Punkte bilden eine Art Paraboloid (eine\n",
    "schüsselförmige 3D-Fläche). In dieser neuen Ansicht können wir eine Ebene\n",
    "finden, die die roten von den blauen Punkten trennt.\n",
    "\n",
    "In der folgenden Grafik ist eine Trennebene eingezeichnet. Wenn wir nun den\n",
    "Schnitt der Trennebene mit dem Paraboloid bilden, entsteht eine Kreislinie.\n",
    "Drehen wir wieder unsere Ansicht zurück, so dass wir von oben auf die\n",
    "X1-X2-Feature-Ebene blicken, so ist dieser Kreis genau das, was wir auch als\n",
    "Menschen genommen hätten, um die roten von den blauen Punkten zu trennen.\n",
    "\n",
    "![https://gramschs.github.io/book_ml4ing/_images/fig10_06_with_plane.png](https://gramschs.github.io/book_ml4ing/_images/fig10_06_with_plane.png)\n",
    "\n",
    "![https://gramschs.github.io/book_ml4ing/_images/fig10_07_with_circle.png](https://gramschs.github.io/book_ml4ing/_images/fig10_07_with_circle.png)\n",
    "\n",
    "## Kernel-Trick\n",
    "\n",
    "Bei diesem künstlichen Datensatz haben die Abstände zum Ursprung als neues\n",
    "Feature sehr gut funktioniert. Das lag daran, dass die Punkte tatsächlich in\n",
    "Kreisen um den Ursprung verteilt waren. Was ist, wenn das nicht der Fall ist?\n",
    "Wenn der Schwerpunkt der Kreise verschoben wäre, müssten wir auch die\n",
    "Transformationsfunktion zum Erzeugen des dritten Features in diesen Schwerpunkt\n",
    "verschieben.\n",
    "\n",
    "Glücklicherweise übernimmt Scikit-Learn für uns die Suche nach einer passenden\n",
    "Transformationsfunktion automatisch. Das Verfahren, das dazu in die\n",
    "SVM-Algorithmen eingebaut ist, wird **Kernel-Trick** genannt. Es beruht darauf,\n",
    "dass manche Funktionen in ein Skalarprodukt umgewandelt werden können. Und dann\n",
    "wird nicht das dritte Feature mit der Transformationsfunktion aus den ersten\n",
    "beiden Features berechnet, was sehr zeitaufwendig werden könnte, sondern die\n",
    "Transformationsfunktion wird direkt in das Lernverfahren eingebaut. Funktionen,\n",
    "die dafür geeignet sind, werden als **Kernel-Funktionen** bezeichnet.\n",
    "\n",
    "Am häufigsten kommt dabei die sogenannte **radiale Basisfunktion** zum Einsatz.\n",
    "Die radialen Basisfunktionen werden mit **RBF** abgekürzt. Sie haben die\n",
    "wichtige Eigenschaft, dass sie nur vom Abstand eines Punktes zum Ursprung\n",
    "abhängen; so wie unser Beispiel oben.\n",
    "\n",
    "Um nichtlinear trennbare Daten zu klassifizieren, nutzen wir in Scikit-Learn das\n",
    "SVC-Lernverfahren. Doch diesmal wählen wir als Kern nicht die linearen\n",
    "Funktionen, sondern die sogenannten radialen Basisfunktionen RBF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_modell = svm.SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abca1adf",
   "metadata": {},
   "source": [
    "Erneut sind die beiden Merkmale im gleichen Bereich, so dass wir direkt die\n",
    "Trennung in Trainings- und Testdaten vollziehen können, ohne die Daten zu\n",
    "skalieren. Danach erfolgt das Training wie gewohnt mit der `fit()`-Methode, die\n",
    "Bewertung mit der `score()`-Methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78243698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trennung in Trainings- und Testdaten\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Training\n",
    "svm_modell.fit(X_train, y_train);\n",
    "\n",
    "# Bewertung\n",
    "score_train = svm_modell.score(X_train,y_train)\n",
    "score_test = svm_modell.score(X_test,y_test)\n",
    "\n",
    "print('Score Trainingsdaten: {:.2f}'.format(score_train))\n",
    "print('Score Testdaten: {:.2f}'.format(score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85662f45",
   "metadata": {},
   "source": [
    "Sowohl für die Trainings- als auch Testdaten haben wir einen Score von 1.0. Die\n",
    "Trennung hat perfekt funktioniert. Zum Vergleich können wir ein lineares SVM\n",
    "trainieren und bewerten lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zum Vergleich: lineares SVM\n",
    "svm_linear = svm.SVC(kernel='linear')\n",
    "svm_linear.fit(X_train, y_train)\n",
    "score_train = svm_linear.score(X_train,y_train)\n",
    "score_test = svm_linear.score(X_test,y_test)\n",
    "\n",
    "print('Score Trainingsdaten lineares SVM-Modell: {:.2f}'.format(score_train))\n",
    "print('Score Testdaten lineares SVM-Modell: {:.2f}'.format(score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749fcab",
   "metadata": {},
   "source": [
    "Die Scores des linearen SVM-Modells (0.37 für die Trainingsdaten und 0.32 für\n",
    "die Testdaten) zeigen, dass das lineare SVM-Modell nicht in der Lage ist, die\n",
    "blauen von den roten Punkten zu trennen bzw. die Kategorien `0` oder `1` korrekt\n",
    "zu klassifizieren.\n",
    "\n",
    "Aber wie sieht nun die Trennung aus? Wir können erneut die Funktion\n",
    "`plot_svc_grenze()`aus dem vorherigen Abschnitt nutzen, um die Stützvektoren mit\n",
    "einem orangefarbenem Kreis zu markieren und die Entscheidungsgrenze zu\n",
    "visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ff472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelle: VanderPlas \"Data Science mit Python\", S. 482\n",
    "# modified by Simone Gramsch\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt; plt.style.use('bmh')\n",
    "\n",
    "def plot_svc_grenze(model):\n",
    "    # aktuelles Grafik-Fenster auswerten\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # Raster für die Auswertung erstellen\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "\n",
    "    # Entscheidungsgrenzen und Margin darstellen\n",
    "    ax.contour(X, Y, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "    # Stützvektoren darstellen\n",
    "    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, linewidth=1, facecolors='none', edgecolors='orange');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X1, X2, c=y, cmap='coolwarm')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_title('Künstliche Messdaten');\n",
    "plot_svc_grenze(svm_modell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db1484",
   "metadata": {},
   "source": [
    "Mit den radialen Basisfunktionen passt sich die Entscheidungsgrenze flexibel an\n",
    "die Daten an. Statt eines perfekten Kreises erhalten wir ein deformiertes Ei,\n",
    "das sehr gut zu den Daten passt.\n",
    "\n",
    "## Zusammenfassung\n",
    "\n",
    "In diesem Abschnitt haben wir uns mit nichtlinearen Support Vector Machines\n",
    "beschäftigt. Die Idee zur Klassifizierung nichtlinearer Daten ist, ein neues\n",
    "Feature hinzuzufügen. Mathematisch gesehen projizieren wir also die Daten mit\n",
    "einer nichtlinearen Transformationsfunktion in einen höherdimensionalen Raum und\n",
    "trennen sie in dem höherdimensionalen Raum. Dann kehren wir durch den Schnitt der\n",
    "Trennebene mit der Transformationsfunktion wieder in den ursprünglichen Raum\n",
    "zurück. Wenn wir als Transformationsfunktion die sogenannten Kernel-Funktionen\n",
    "verwenden, können wir auf die Transformation der Daten verzichten und die\n",
    "Transformation direkt in die SVM einbauen. Das wird Kernel-Trick genannt und\n",
    "sorgt für die Effizienz und damit Beliebtheit von SVMs."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
