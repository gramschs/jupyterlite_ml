{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1790ab",
   "metadata": {},
   "source": [
    "# 9.3 XGBoost\n",
    "\n",
    "In der bisherigen Vorlesung haben wir vor allem Pandas und Scikit-Learn benutzt.\n",
    "Zwar bietet Scikit-Learn Boosting-Verfahren an, in vielen Wettbewerben hat sich\n",
    "jedoch eine andere Bibliothek durchgesetzt, die eine optimierte Variante des\n",
    "Stochastic Gradient Boosting anbietet: **XGBoost**.\n",
    "\n",
    ":class: warning\n",
    "Falls bei Ihnen XGBoost nicht installiert sein sollte, folgen Sie bitte den\n",
    "Anweisungen auf der Internetseite\n",
    "[https://xgboost.readthedocs.io](https://xgboost.readthedocs.io/en/stable/install.html)\n",
    "und installieren Sie XGBoost jetzt nach.\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "* Sie können XGBoost für Regressions- und Klassifikationsaufgaben einsetzen.\n",
    "* Sie wissen, wie Sie mit Analysen der Maßzahlen Fehler und Log Loss für\n",
    "  Trainings- und Testdaten beurteilen können, ob Überanpassung (Overfitting)\n",
    "  vorliegt.\n",
    "* Sie kennen die Methode **Frühes Stoppen** zur Reduzierung der Überanpassung\n",
    "  (Overfitting).\n",
    "* Sie wissen, dass XGBoost nicht manuell feinjustiert werden sollte, sondern mit\n",
    "  Gittersuche oder weiteren Bibliotheken (z.B. Optuna).\n",
    "\n",
    "## XGBoost benutzt Scikit-Learn API\n",
    "\n",
    "In einem früheren Kapitel haben wir Stochastic Gradient Boosting theoretisch\n",
    "kennengelernt: Dabei werden sequentiell Modelle trainiert, die jeweils die\n",
    "Fehler des Vorgängermodells korrigieren. XGBoost ist eine hochoptimierte und\n",
    "erweiterte Implementierung dieses Verfahrens.\n",
    "\n",
    "XGBoost steht für e**X**treme **G**radient **Boost**ing und ist aus\n",
    "Performancegründen in der Programmiersprache C++ implementiert. Für\n",
    "Python-Programmierer wurde ein Python-Modul mit dem Ziel geschaffen, die\n",
    "gleichen Schnittstellen wie Scikit-Learn anzubieten, so dass kaum\n",
    "Einarbeitungszeit in eine neue Bibliothek erforderlich ist. Vor allem benötigen\n",
    "Data Scientists auch keine C++\\-Programmierkenntnisse, sondern können weiterhin\n",
    "mit Python arbeiten.\n",
    "\n",
    "Wir bleiben bei unserem Beispiel mit der Verkaufsaktion im Autohaus aus dem\n",
    "vorherigen Kapitel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Erzeugung künstlicher Daten\n",
    "X_array, y_array = make_moons(n_samples=120, random_state=0, noise=0.3)\n",
    "\n",
    "daten = pd.DataFrame({\n",
    "    'Kilometerstand [km]': 10000 * (X_array[:,0] + 2),\n",
    "    'Preis [EUR]': 5000 * (X_array[:,1] + 2),\n",
    "    'verkauft': y_array,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e52e27f",
   "metadata": {},
   "source": [
    "XGBoost kann Pandas DataFrames nicht verarbeiten, sondern benötigt die reinen\n",
    "Zahlenwerte in Form von Matrizen. Das ist in der Tat kein Problem, denn die\n",
    "Datenstruktur DataFrame stellt die reinen Matrizen über die Methode `.values`\n",
    "direkt zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaption der Daten\n",
    "X = daten[['Kilometerstand [km]', 'Preis [EUR]']].values\n",
    "y = daten['verkauft'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e62a51",
   "metadata": {},
   "source": [
    "Als nächstes importieren wir XGBoost. Es ist üblich, das ganze Modul zu\n",
    "importieren und mit `xgb` abzukürzen. Danach initialisieren wir das\n",
    "Klassifikationsmodell `XGBClassifier` und trainieren es auf den Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb \n",
    "\n",
    "modell = xgb.XGBClassifier()\n",
    "modell.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef38308d",
   "metadata": {},
   "source": [
    "Als nächstes visualisieren wir die Prognose des trainierten\n",
    "XGBoost-Klassifikators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d443b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "my_colormap = ListedColormap(['#EF553B33', '#636EFA33'])\n",
    "fig = DecisionBoundaryDisplay.from_estimator(modell, X,  cmap=my_colormap)\n",
    "fig.ax_.scatter(X[:,0], X[:,1], c=y, cmap=my_colormap)\n",
    "fig.ax_.set_xlabel('Kilometerstand [km]');\n",
    "fig.ax_.set_ylabel('Preis [EUR]');\n",
    "fig.ax_.set_title('XGBoost: Entscheidungsgrenzen');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eedd3a",
   "metadata": {},
   "source": [
    "Die Entscheidungsgrenzen sehen sehr plausibel aus.\n",
    "\n",
    "Genau wie beim Random Forest können wir uns die Feature Importance ausgeben\n",
    "lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance wie bei Random Forest\n",
    "import plotly.express as px\n",
    "\n",
    "feature_importance = pd.Series(\n",
    "    modell.feature_importances_, \n",
    "    index=['Kilometerstand', 'Preis']\n",
    ")\n",
    "\n",
    "fig = px.bar(feature_importance, orientation='h',\n",
    "    title='Feature Importance bei XGBoost',\n",
    "    labels={'value': 'Wichtigkeit', 'index': 'Merkmal'})\n",
    "fig.update_traces(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d586a",
   "metadata": {},
   "source": [
    "Der Preis (0.57) hat eine um den Faktor 1.3 höhere Feature Importance als der\n",
    "Kilometerstand (0.43). Allerdings liegen hier aus didaktischen Gründen nur\n",
    "künstliche Daten vor, so dass diese Erkenntnis nicht auf die reale Welt\n",
    "verallgemeinerbar ist.\n",
    "\n",
    "## XGBoost neigt stark zur Überanpassung (Overfitting)\n",
    "\n",
    "XGBoost neigt zu Überanpassung (Overfitting), weil es sehr komplexe Modelle\n",
    "durch Hinzufügen vieler Bäume erstellt, die sich immer stärker an die\n",
    "Trainingsdaten anpassen. Um das an unserem Beispiel mit der Verkaufsaktion im\n",
    "Autohaus zu zeigen, fügen wir noch neue, unbekannte Testdaten hinzu. Dazu\n",
    "verdoppeln wir die Anzahl der Autos (`n_samples=2000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugung künstlicher Daten\n",
    "X_array, y_array = make_moons(n_samples=2000, random_state=0, noise=0.3)\n",
    "\n",
    "daten = pd.DataFrame({\n",
    "    'Kilometerstand [km]': 10000 * (X_array[:,0] + 2),\n",
    "    'Preis [EUR]': 5000 * (X_array[:,1] + 2),\n",
    "    'verkauft': y_array,\n",
    "    })\n",
    "\n",
    "X = daten[['Kilometerstand [km]', 'Preis [EUR]']].values\n",
    "y = daten['verkauft'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7d4c8",
   "metadata": {},
   "source": [
    "Anschließend teilen wir die 2000 Autos in zwei Gruppen: Trainings- und\n",
    "Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0041bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33021d24",
   "metadata": {},
   "source": [
    "Diesmal legen wir explizit fest, aus wie vielen Modellen das Boosting-Verfahren\n",
    "bestehen soll. Dazu setzen wir `n_estimators=200`. Oft wird auch von der Anzahl\n",
    "der »Boosting-Runden« gesprochen. Das Training auf den Trainingsdaten liefert\n",
    "ein sehr gutes Ergebnis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "modell = xgb.XGBClassifier(n_estimators=200)\n",
    "\n",
    "modell.fit(X_train, y_train)\n",
    "\n",
    "score_train = modell.score(X_train, y_train)\n",
    "print(f'Score bezogen auf Trainingsdaten: {score_train:.2f}')\n",
    "score_test = modell.score(X_test, y_test)\n",
    "print(f'Score bezogen auf Testdaten: {score_test:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09baf280",
   "metadata": {},
   "source": [
    "Die Trainingsdaten werden perfekt prognostiziert. Auch bei den Testdaten\n",
    "erhalten wir ein gutes Ergebnis, das aber im Vergleich zu dem sehr guten Score\n",
    "bei den Trainingsdaten abfällt. Es fällt schwer, zu entscheiden, ob eine\n",
    "Überanpassung (Overfitting) vorliegt. XGBoost ist ein iteratives Verfahren.\n",
    "Zunächst wird Modell Nr. 1 trainiert, darauf aufbauend Modell Nr. 2 usw. Wir\n",
    "wiederholen jetzt das Training des XGBoost-Klassifikators, aber lassen durch ein\n",
    "weiteres Argument mitprotokollieren, was in den einzelnen Boosting-Runden (=\n",
    "Iterationen) passiert.\n",
    "\n",
    "Zuerst legen wir fest, welche internen Bewertungskennzahlen (= Metrik, Maßzahl)\n",
    "mitprotokolliert werden sollen. Wir wählen als erste Maßzahl den Fehler, also\n",
    "die relative Anzahl der falsch klassifizierten Autos. Die zweite Maßzahl ist die\n",
    "Log Loss, die nicht nur bewertet, ob die Klassifikation richtig ist, sondern\n",
    "auch wie sicher das Modell bei seiner Vorhersage ist.\n",
    "\n",
    "Technisch setzen wir dies um, indem wir bei der Initialisierung des\n",
    "XGBoost-Modells das optionale Argument `eval_metric=['error', 'logloss']`\n",
    "setzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6cf610",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell = xgb.XGBClassifier(n_estimators=200, eval_metric=['error', 'logloss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0cf04",
   "metadata": {},
   "source": [
    "Allerdings ist damit noch nicht festgelegt, auf welchen Daten die Fehler-Maßzahl\n",
    "und die Log-Loss-Maßzahl berechnet werden. Zunächst sollen beide Maßzahlen für\n",
    "die Trainingsdaten berechnet werden, dann für die Testdaten. Das erreichen wir\n",
    "mit dem optionalen Argument `eval_set=`, dem wir folgendermaßen die Trainings-\n",
    "und Testdaten mitgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab88383",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe098aab",
   "metadata": {},
   "source": [
    "Wir setzen noch `verbose=False`, damit nicht für jedes Modell bzw. jede\n",
    "Boosting-Runde die vier Maßzahlen auf dem Bildschirm ausgegeben werden. Nach dem\n",
    "Training können wir die vier Maßzahlen mit der Methode `.evals_result()` aus dem\n",
    "trainierten Modell extrahieren. Um die Maßzahlen zu visualisieren, packen wir\n",
    "sie in einen Pandas-DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c29978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "masszahlen = modell.evals_result()\n",
    "metriken = pd.DataFrame({\n",
    "    'Fehler Train': masszahlen['validation_0']['error'],\n",
    "    'Fehler Test': masszahlen['validation_1']['error'],\n",
    "    'Log Loss Train': masszahlen['validation_0']['logloss'],\n",
    "    'Log Loss Test': masszahlen['validation_1']['logloss']\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e791d5",
   "metadata": {},
   "source": [
    "Wir visualisieren Fehler und Log Loss getrennt voneinander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Fehler plotten\n",
    "fig = px.line(metriken[['Fehler Train', 'Fehler Test']],\n",
    "    title='Fehler in jeder Boosting-Runde',\n",
    "    labels={'value': 'Fehler', 'index': 'Boosting-Runde', 'variable': 'Legende'})\n",
    "fig.show()\n",
    "\n",
    "# Log Loss plotten\n",
    "fig = px.line(metriken[['Log Loss Train', 'Log Loss Test']],\n",
    "    title='Log Loss in jeder Boosting-Runde',\n",
    "    labels={'value': 'Log Loss', 'index': 'Boosting-Runde', 'variable': 'Legende'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336185a",
   "metadata": {},
   "source": [
    "Der Fehler bei den Trainingsdaten wird von Boosting-Runde zu Boosting-Runde\n",
    "kleiner, aber der Fehler der Testdaten wächst. Zunächst wird der Fehler der\n",
    "Testdaten kleiner, erreicht in Minimum in der 6. Boosting-Runde, um dann wieder zu\n",
    "steigen. Dieses Verhalten ist typisch für Überanpassung (Overfitting). Etwas\n",
    "deutlicher wird dieses Phänomen, wenn wir uns die (transformierte) Differenz\n",
    "der Wahrscheinlichkeiten ansehen, die Log-Loss-Maßzahl.\n",
    "\n",
    "Am kleinsten ist die Log-Loss-Maßzahl für die Boosting-Runde 9, danach steigt\n",
    "die Log-Loss-Maßzahl wieder an. Am besten wäre es nach dieser Analyse gewesen,\n",
    "nach der 6. oder 9. Boosting-Runde aufzuhören, da dann die Überanpassung\n",
    "(Overfitting) an die Trainingsdaten einsetzt.\n",
    "\n",
    "## Bekämpfen von Überanpassung (Overfitting)\n",
    "\n",
    "Es gibt einige Hyperparameter von XGBoost, die helfen, Überanpassung\n",
    "(Overfitting) zu reduzieren. Eine Möglichkeit ist es, früher zu stoppen und\n",
    "nicht die voreingestellte Anzahl an Modellen bzw. Boosting-Runden (Iterationen)\n",
    "zu durchlaufen. Das wird durch das optionale Argument `early_stopping_rounds=`\n",
    "im Konstruktor ermöglicht. Die Zahl, die diesem Parameter übergeben wird, gibt\n",
    "die Anzahl der Boosting-Runden vor, nach denen gestoppt wird, falls sich kaum\n",
    "etwas an der Maßzahl geändert hat.\n",
    "\n",
    "Wichtig: In der Praxis sollte early stopping nicht auf den Testdaten erfolgen,\n",
    "sondern auf einem separaten Validierungsset, um Data Leakage zu vermeiden. Für\n",
    "dieses didaktische Beispiel verwenden wir vereinfacht die Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2894e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell = xgb.XGBClassifier(n_estimators=200, early_stopping_rounds=10, eval_metric=['error', 'logloss'])\n",
    "modell.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e39e140",
   "metadata": {},
   "source": [
    "Visualisiert sieht die Log-Loss-Statistik für das obige Beispiel so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ef4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "masszahlen = modell.evals_result()\n",
    "metriken = pd.DataFrame({\n",
    "    'Fehler Train': masszahlen['validation_0']['error'],\n",
    "    'Fehler Test': masszahlen['validation_1']['error'],\n",
    "    'Log Loss Train': masszahlen['validation_0']['logloss'],\n",
    "    'Log Loss Test': masszahlen['validation_1']['logloss']\n",
    "    })\n",
    "\n",
    "fig = px.line(metriken[['Fehler Train', 'Fehler Test']],\n",
    "    title='Frühes Stoppen: Fehler',\n",
    "    labels={'value': 'Fehler', 'index': 'Boosting-Runde', 'variable': 'Legende'})\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(metriken[['Log Loss Train', 'Log Loss Test']],\n",
    "    title='Frühes Stoppen: Log Loss',\n",
    "    labels={'value': 'Log Loss', 'index': 'Boosting-Runde', 'variable': 'Legende'})\n",
    "fig.show()\n",
    "\n",
    "print(f'Training gestoppt nach {modell.best_iteration + 1} Boosting-Runde(n)')\n",
    "print(f'Bester Score auf Testdaten: {modell.best_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c3807",
   "metadata": {},
   "source": [
    "Eine weitere Möglichkeit, Überanpassung (Overfitting) zu reduzieren, besteht\n",
    "darin, die Tiefe der Entscheidungsbäume zu begrenzen. Wir benutzen\n",
    "Entscheidungsbaum-Stümpfe, die nur einen Split haben. Das erreichen wir mit dem\n",
    "optionalen Argument `max_depth=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89832545",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell = xgb.XGBClassifier(max_depth=1, n_estimators=200, eval_metric=['error', 'logloss'])\n",
    "modell.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)\n",
    "\n",
    "masszahlen = modell.evals_result()\n",
    "metriken = pd.DataFrame({\n",
    "    'Fehler Train': masszahlen['validation_0']['error'],\n",
    "    'Fehler Test': masszahlen['validation_1']['error'],\n",
    "    'Log Loss Train': masszahlen['validation_0']['logloss'],\n",
    "    'Log Loss Test': masszahlen['validation_1']['logloss']\n",
    "    })\n",
    "\n",
    "fig = px.line(metriken[['Fehler Train', 'Fehler Test']],\n",
    "    title='Begrenzte Entscheidungsbäume: Fehler',\n",
    "    labels={'value': 'Fehler', 'index': 'Boosting-Runde', 'variable': 'Legende'})\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(metriken[['Log Loss Train', 'Log Loss Test']],\n",
    "    title='Begrenzte Entscheidungsbäume: Log Loss',\n",
    "    labels={'value': 'Log Loss', 'index': 'Boosting-Runde', 'variable': 'Legende'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e768b",
   "metadata": {},
   "source": [
    "Es gibt noch einige weitere Hyperparameter, die für \"das\" beste Modell\n",
    "feinjustiert werden können. Händisch gelingt es kaum, alle Hyperparameter\n",
    "optimal einzustellen, so dass hier eine Gittersuche oder gar eine Bibliothek wie\n",
    "[Optuna](https://github.com/optuna/optuna) eingesetzt werden sollte. Das\n",
    "übersteigt jedoch den zeitlichen Rahmen dieser Vorlesung und wird daher hier\n",
    "nicht behandelt.\n",
    "\n",
    "### Vergleich: Random Forest vs. XGBoost\n",
    "\n",
    "Zum Abschluss dieses Kapitels beschäftigen wir uns noch mit einem Vergleich der beiden Verfahren Random Forest und XGBoost.\n",
    "\n",
    "| Aspekt | Random Forest | XGBoost |\n",
    "|--------|---------------|---------|\n",
    "| Training | parallel | sequentiell |\n",
    "| Overfitting | weniger anfällig | stark anfällig |\n",
    "| Hyperparameter-Tuning | wenig nötig | intensiv nötig |\n",
    "| Geschwindigkeit | schnell | langsamer |\n",
    "| Typische Genauigkeit | gut | sehr gut |\n",
    "\n",
    "Random Forests eignen sich gut für einen schnellen ersten Ansatz, während\n",
    "XGBoost durch sorgfältiges Tuning oft bessere Ergebnisse liefert, aber mehr\n",
    "Aufwand erfordert.\n",
    "\n",
    "## Zusammenfassung und Ausblick\n",
    "\n",
    "Mit XGBoost haben Sie ein ML-Modell für das überwachte Lernen kennengelernt, das\n",
    "in den vergangenen Jahren sehr viele Wettbewerbe beispielsweise auf der\n",
    "Plattform Kaggle gewonnen hat. Die Mächtigkeit der Algorithmen führt aber häufig\n",
    "zur Überanpassung (Overfitting), so dass die sorgsame Feinjustierung der\n",
    "Hyperparameter besonders wichtig ist."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
