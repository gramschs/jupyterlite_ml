{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a526fa",
   "metadata": {},
   "source": [
    "# 9.2 Random Forests\n",
    "\n",
    "Im letzten Kapitel haben wir verschiedene Ensemble-Methoden in der Theorie\n",
    "kennengelernt: Stacking, Bagging und Boosting. Für die beiden letzteren\n",
    "Ensemble-Methoden werden besonders häufig Entscheidungsbäume (Decision Trees)\n",
    "eingesetzt. Daher betrachten wir in diesem Kapitel die praktische Umsetzung von\n",
    "Bagging mit Entscheidungsbäumen, die sogenannten Random Forests.\n",
    "\n",
    "* Sie können das ML-Modell **Random Forest** in der Praxis anwenden.\n",
    "* Sie können mit Hilfe der **Feature Importance** bewerten, wie groß der\n",
    "  Einfluss eines Merkmals auf die Prognosegenauigkeit des Random Forests ist.\n",
    "\n",
    "## Random Forests mit Scikit-Learn\n",
    "\n",
    "Entscheidungsbäume (Decision Trees) haben wir bereits betrachtet. Sie sind\n",
    "aufgrund ihrer Einfachheit und vor allem aufgrund ihrer Interpretierbarkeit sehr\n",
    "beliebt. Allerdings ist ihre Tendenz zum Overfitting problematisch. Daher\n",
    "kombinieren wir die Ensemble-Methode Bagging mit Entscheidungsbäumen (Decision\n",
    "Trees). Indem aus den Trainingsdaten zufällig Bootstrap-Stichproben ausgewählt\n",
    "werden, erhalten wir unterschiedliche Entscheidungsbäume (Decision Trees).\n",
    "Zusätzlich wird beim Training der Entscheidungsbäume nicht mit allen Merkmalen\n",
    "(Features) trainiert. Bei jedem Split eines Baumes wird nur eine zufällige\n",
    "Teilmenge der Merkmale betrachtet, um die beste Trennung zu finden. Durch diese\n",
    "zwei Maßnahmen wird die Anpassung der Entscheidungsbäume an die Trainingsdaten\n",
    "(Overfitting) reduziert.\n",
    "\n",
    "Um den Random Forest von Scikit-Learn praktisch auszuprobieren, erzeugen wir\n",
    "künstliche Daten. Dazu verwenden wir die Funktion `make_moons` von Scikit-Learn,\n",
    "die Zufallszahlen generiert und interpretieren die Zufallszahlen als\n",
    "Kilometerstände und Preise von Autos bei einer fiktiven Verkaufsaktion.\n",
    "Zusätzlich lassen wir zufällig Nullen und Einsen erzeugen, die wir als\n",
    "»verkauft« oder »nicht verkauft« interpretieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Erzeugung künstlicher Daten\n",
    "X_array, y_array = make_moons(n_samples=120, random_state=0, noise=0.3)\n",
    "\n",
    "daten = pd.DataFrame({\n",
    "    'Kilometerstand [km]': 10000 * (X_array[:,0] + 2),\n",
    "    'Preis [EUR]': 5000 * (X_array[:,1] + 2),\n",
    "    'verkauft': y_array,\n",
    "    })\n",
    "\n",
    "# Adaption der Daten\n",
    "X = daten[['Kilometerstand [km]', 'Preis [EUR]']].values\n",
    "y = daten['verkauft'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770c678",
   "metadata": {},
   "source": [
    "Diesmal werden in dem Autohaus 120 Autos zum Verkauf angeboten (siehe Option\n",
    "`n_samples=120`). Nach Aktionsende werden die Merkmale Kilometerstand und Preis\n",
    "tabellarisch erfasst und notiert, ob das Auto verkauft wurde (True bzw. 1) oder\n",
    "nicht verkauft wurde (False bzw. 0). Wir visualisieren die Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9a320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# plot artificial data\n",
    "fig = px.scatter(daten, x = 'Kilometerstand [km]', y = 'Preis [EUR]', color=daten['verkauft'].astype(bool),\n",
    "        title='Künstliche Daten: Verkaufsaktion Autohaus',\n",
    "        labels = {'x': 'Kilometerstand [km]', 'y': 'Preis [EUR]', 'color': 'verkauft'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e308c4",
   "metadata": {},
   "source": [
    "Nachdem wir die Vorbereitungen für die Daten abgeschlossen haben, können wir\n",
    "Scikit-Learn einen Random Forest trainieren lassen. Dazu importieren wir den\n",
    "Algorithmus aus dem Modul `ensemble`. Da der Random Forest ein Ensemble von\n",
    "Entscheidungsbäumen (Decision Trees) ist, haben wir nun die Möglichkeit, die\n",
    "Anzahl der Entscheidungsbäume festzulegen. Voreingestellt sind 100\n",
    "Entscheidungsbäume. Aus didaktischen Gründen reduzieren wir diese Anzahl auf\n",
    "vier und setzen das Argument `n_estimators=` auf `4`. Ebenfalls aus didaktischen\n",
    "Gründen fixieren wir die Zufallszahlen, mit Hilfe derer das Bootstrapping und\n",
    "die Auswahl der Merkmale (Features) umgesetzt wird, mit `random_state=0`. In\n",
    "einem echten Projekt würden wir das unterlassen. Zuletzt führen wir das Training\n",
    "mit der `.fit()`-Methode durch. Weitere Details finden Sie unter [Scikit-Learn\n",
    "Dokumentation →\n",
    "RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_random_forest = RandomForestClassifier(n_estimators=4, random_state=0)\n",
    "model_random_forest.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec0241",
   "metadata": {},
   "source": [
    "Als nächstes lassen wir den Random Forest für jeden Punkt des Gebiets\n",
    "prognostizieren, ob ein Auto mit diesem Kilometerstand und diese Preis\n",
    "verkaufbar wäre oder nicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "my_colormap = ListedColormap(['#EF553B33', '#636EFA33'])\n",
    "fig = DecisionBoundaryDisplay.from_estimator(model_random_forest, X,  cmap=my_colormap)\n",
    "fig.ax_.scatter(X[:,0], X[:,1], c=y, cmap=my_colormap)\n",
    "fig.ax_.set_xlabel('Kilometerstand [km]');\n",
    "fig.ax_.set_ylabel('Preis [EUR]');\n",
    "fig.ax_.set_title('Random Forest: Entscheidungsgrenzen');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35f5f9",
   "metadata": {},
   "source": [
    "Möchte man die vier Entscheidungsbäume (Decision Trees) analysieren, aus denen\n",
    "der Random Forest kombiniert wurde, kann man mit dem Attribut `estimators_`\n",
    "darauf zugreifen. Wir lassen uns jetzt die Entscheidungsgrenzen einzeln für\n",
    "jeden Entscheidungsbaum anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_colormap = ListedColormap(['#EF553B33', '#636EFA33'])\n",
    "for (nummer, baum) in zip(range(4), model_random_forest.estimators_):\n",
    "    fig = DecisionBoundaryDisplay.from_estimator(baum, X,  cmap=my_colormap)\n",
    "    fig.ax_.scatter(X[:,0], X[:,1], c=y, cmap=my_colormap)\n",
    "    fig.ax_.set_xlabel('Kilometerstand [km]');\n",
    "    fig.ax_.set_ylabel('Preis [EUR]');\n",
    "    fig.ax_.set_title(f'Entscheidungsbaum {nummer+1}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f473f",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Der Random Forest reduziert das Overfitting und ist damit für zukünftige\n",
    "Prognosen besser gerüstet, verliert aber seine leichte Interpretierbarkeit. Ein\n",
    "großer Vorteil des Entscheidungsbaumes (Decision Trees) ist ja, dass wir die\n",
    "Entscheidungen als eine Abfolge von Entscheidungsfragen gut nachvollziehen\n",
    "können. Jeder der einzelnen Entscheidungsbäume kommt jedoch zu einer anderen\n",
    "Reihenfolge der Entscheidungsfragen und zu anderen Grenzen. Dafür bietet der\n",
    "Random-Forest-Algorithmus eine alternative Bewertung, wie wichtig einzelne\n",
    "Merkmale (Features) sind, die sogenannte **Feature Importance**.\n",
    "\n",
    "Feature Importance bewertet, wie wichtig der Einfluss eines Merkmals (Features)\n",
    "auf die Prognoseleistung ist. Ist die Feature Importance eines Merkmals\n",
    "(Features) höher, so trägt dieses Merkmal (Feature) auch mehr zu der Genauigkeit\n",
    "der Prognose bei. Bei Entscheidungsbäumen wird für jedes Merkmal (Feature)\n",
    "berechnet, wie groß die Reduktion der Gini-Impurity ist. Gibt es ein Merkmal,\n",
    "das eindeutig die Gini-Impurity reduziert, dann hat dieses Merkmal auch einen\n",
    "großen Einfluss auf die Prognosefähigkeit des Modells. Wir könnten nach dem\n",
    "Training des Entscheidungsbaumes zusammenfassen, wie oft und wieviel ein\n",
    "bestimmtes Merkmal zur Reduktion beiträgt. In der Praxis kommt es aber oft vor,\n",
    "dass bei einem Split mehrere Merkmale gleichermaßen die Gini-Impurity\n",
    "reduzieren. Dann wird eines der Merkmale zufällig ausgewählt. Daher kann es\n",
    "schwierig sein, bei einem Entscheidungsbaum die Feature Importance zu bewerten.\n",
    "Bei einem Random Forest hingegen werden viele Entscheidungsbäume trainiert. Wenn\n",
    "wir jetzt bei allen Entscheidungsbäumen die Feature Importance berechnen und den\n",
    "Mittelwert bilden, erhalten wir ein aussagekräftiges Bewertungskriterium, wie\n",
    "stark einzelne Merkmale die Prognosefähigkeit beeinflussen.\n",
    "\n",
    "Wir trainieren nun einen Random Forest mit der Standardeinstellung von 100\n",
    "Entscheidungsbäumen und lassen uns dann die Feature Importance ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X,y)\n",
    "\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d643c5b1",
   "metadata": {},
   "source": [
    "Der erste Wert gibt die Feature Importance für das erste Merkmal an und der\n",
    "zweite Wert entsprechend für das zweite Merkmal. Es ist üblich, die Feature\n",
    "Importance als Balkendiagramm zu visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d91099",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(model.feature_importances_, index=['Kilometerstand [km]', 'Preis [EUR]'])\n",
    "\n",
    "fig = px.bar(feature_importances, orientation='h',\n",
    "  title='Verkaufsaktion im Autohaus', \n",
    "  labels={'value':'Feature Importance', 'index': 'Merkmal'})\n",
    "fig.update_traces(showlegend=False) \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4896c1",
   "metadata": {},
   "source": [
    "Der Preis ist demnach wichtiger als der Kilometerstand (wobei es hier ja ein\n",
    "künstliches Beispiel ist).\n",
    "\n",
    "## Zusammenfassung und Ausblick\n",
    "\n",
    "Random Forests sind einfachen Entscheidungsbäumen vorzuziehen, da sie das\n",
    "Overfitting reduzieren. Die Erzeugung der einzelnen Entscheidungsbäume kann\n",
    "parallelisiert werden, so dass das Training eines Random Forests sehr schnell\n",
    "durchgeführt werden kann. Auch für große Datenmengen mit sehr unterschiedlichen\n",
    "Eigenschaften arbeitet der Random Forest sehr effizient. Er ermöglicht auch eine\n",
    "Interpretation, welche Eigenschaften ggf. einen größeren Einfluss haben als\n",
    "andere Eigenschaften."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
