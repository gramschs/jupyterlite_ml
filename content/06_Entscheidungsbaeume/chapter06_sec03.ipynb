{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7a5ca7",
   "metadata": {},
   "source": [
    "# 6.3 Entscheidungsbäume in der Praxis\n",
    "\n",
    "Entscheidungsbäume bieten viele Vorteile, haben aber auch Nachteile, die wir in\n",
    "diesem Kapitel diskutieren werden. Darüber hinaus lernen wir Methoden kennen,\n",
    "um bei Entscheidungsbäumen diese Nachteile zu reduzieren.\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "* Sie können in eigenen Worten erklären, was **Overfitting** (deutsch:\n",
    "  **Überanpassung**) ist.\n",
    "* Sie wissen, was **Underfitting** bedeutet.\n",
    "* Sie wissen, dass Entscheidungsbäume eine Tendenz zu Overfitting haben und\n",
    "  Maßnahmen zur Reduzierung von Overfitting ergriffen werden müssen.\n",
    "* Sie wissen, was **Hyperparameter** sind.\n",
    "* Sie kennen Hyperparameter der Entscheidungsbäume wie beispielsweise\n",
    "  * maximale Baumtiefe,\n",
    "  * minimale Anzahl an Datenpunkten in Knoten oder\n",
    "  * minimale Anzahl an Datenpunkten in Blättern.\n",
    "* Sie können die Hyperparameter zum **Prä-Pruning** (deutsch: vorab\n",
    "  Zurechtschneiden) geeignet wählen.\n",
    "\n",
    "## Die Tendenz von Entscheidungsbäumen zum Overfitting\n",
    "\n",
    "Entscheidungsbaummodelle bieten zahlreiche Vorteile. Ein wesentlicher Vorzug ist\n",
    "die Möglichkeit, den trainierten Entscheidungsbaum zu visualisieren, wodurch es\n",
    "leicht nachvollziehbar wird, welche Merkmale einen signifikanten Einfluss haben.\n",
    "Ein weiterer Vorteil ist ihre Effizienz bei heterogenen Daten; sowohl numerische\n",
    "als auch kategoriale Eigenschaften können problemlos verarbeitet werden.\n",
    "Entscheidungsbäume sind selbst bei unterschiedlichen Datenskalen robust und\n",
    "erfordern nur wenig Vorverarbeitung.\n",
    "\n",
    "Trotz dieser Stärken besitzen Entscheidungsbäume eine Neigung zum\n",
    "**Overfitting**. Overfitting, auch als Überanpassung bekannt, beschreibt ein\n",
    "Problem im maschinellen Lernen, bei dem ein Modell die Trainingsdaten zu genau\n",
    "lernt. Das klingt zunächst gut, aber das Modell kann dadurch seine Fähigkeit\n",
    "verlieren, Vorhersagen für neue, unbekannte Daten zu treffen. Im Gegensatz dazu\n",
    "steht das **Underfitting**, das eine zu geringe Anpassung an die Daten bedeutet\n",
    "und ebenfalls unerwünscht ist.\n",
    "\n",
    "Um uns das Problem des Overfittings zu veranschaulichen, betrachten wir erneut\n",
    "das Autohaus-Beispiel, aber diesmal mit mehr Autos. Wir lassen die Autos diesmal\n",
    "mit einer in Scikit-Learn eingebauten Funktion zur Generierung von künstlichen\n",
    "Daten erzeugen, der sogenannten `make_moons`-Funktion (siehe [Dokumentation\n",
    "Scikit-Learn →\n",
    "make_moons](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html))\n",
    "aus dem Module `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a8b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons \n",
    "\n",
    "X_array, y_array = make_moons(noise = 0.5, n_samples=50, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff51da2",
   "metadata": {},
   "source": [
    "Damit die künstlichen Daten besser zu dem Autohaus-Beispiel passen,\n",
    "transformieren wir sie und nutzen die Pandas-Datenstrukturen, um sie effizient\n",
    "zu verwalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Transformation der Merkmalswerte in einen positiven Bereich und \n",
    "# Umwandlung in eine Integer-Matrix\n",
    "X_array = X_array + 1.2 * np.abs(np.min(X_array))\n",
    "X_array[:,0] = np.ceil(X_array[:,0] * 30000)\n",
    "X_array[:,1] = np.ceil(X_array[:,1] * 10000)\n",
    "X = pd.DataFrame(X_array, columns=['Kilometerstand [km]', 'Preis [EUR]'], dtype=(int, int))\n",
    "\n",
    "# Zuweisung von True/False basierend auf den Kategorien 1 bzw. 0\n",
    "y_array = (y_array - 1.0) * (-1)\n",
    "y = pd.Series(y_array, name='verkauft', dtype='bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958e4fb",
   "metadata": {},
   "source": [
    "Nach der Datenvorbereitung visualisieren wir diese:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd31c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x = X['Kilometerstand [km]'], y = X['Preis [EUR]'], color=y,\n",
    "    title='Künstliche Daten Autohaus',\n",
    "    labels={'x': 'Kilometerstand [km]', 'y': 'Preis [EUR]', 'color': 'verkauft'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2428ef",
   "metadata": {},
   "source": [
    "Das Training des Entscheidungsbaumes und dessen Visualisierung erledigt der\n",
    "folgende Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b45524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "modell = DecisionTreeClassifier(random_state=0)\n",
    "modell.fit(X,y)\n",
    "\n",
    "plot_tree(modell,\n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387ace7",
   "metadata": {},
   "source": [
    "Die Visualisierung offenbart zahlreiche Verzweigungen und eine schwer lesbare\n",
    "Beschriftung. Die Entscheidungsgrenzen, die im Folgenden mit\n",
    "`DecisionBoundaryDisplay` visualisiert werden, zeigen eine zu starke Anpassung\n",
    "an die Trainingsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "fig = DecisionBoundaryDisplay.from_estimator(modell, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\n",
    "fig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\n",
    "fig.ax_.set_title('Entscheidungsgrenzen');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98348a4a",
   "metadata": {},
   "source": [
    "Es ist fraglich, ob dieser Entscheidungsbaum nicht zu genau an die\n",
    "Trainingsdaten angepasst wurde. Der dünne blaue vertikale Streifen bei ungefähr\n",
    "97000 km ist wahrscheinlich keine sinnvolle Entscheidung, sondern eher einem\n",
    "Ausreißer geschuldet (dem Auto mit einem Kilometerstand von 97098 km und einem\n",
    "Preis von 28229 EUR). Der Entscheidungsbaum hat sich zu stark an die Daten\n",
    "angepasst. Es ist wahrscheinlich, dass dieser Entscheidungsbaum für Autos mit\n",
    "einem Kilometerstand von ungefähr 97000 km falsche Prognosen treffen wird. Wenn\n",
    "wir mit den gleichen Daten erneut einen Entscheidungsbaum trainieren lassen und\n",
    "den Zufallszahlengenerator mit dem Zustand `random_state=1` initialisieren,\n",
    "erhalten wir ein völlig anderes Ergebnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b5e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell_alternative = DecisionTreeClassifier(random_state=1)\n",
    "modell_alternative.fit(X,y)\n",
    "\n",
    "fig = DecisionBoundaryDisplay.from_estimator(modell_alternative, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\n",
    "fig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\n",
    "fig.ax_.set_title('Entscheidungsgrenzen des alternativen Modells');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83d7a6",
   "metadata": {},
   "source": [
    "Eine Möglichkeit, das Overfitting (Überanpassung) an die Daten zu bekämpfen, ist\n",
    "das Zurechtschneiden (Pruning) der Entscheidungsbäume. Eine andere ist, aus\n",
    "mehreren Entscheidungbäumen einen »durchschnittlichen« Entscheidungsbaum zu\n",
    "bilden. Dieses Verfahren heißt Zufallswald (Random Forest) und wird ausführlich\n",
    "in einem eigenen Kapitel behandelt werden. In diesem Kapitel betrachten wir nur\n",
    "das Zurechtschneiden der Entscheidungsbäume.\n",
    "\n",
    "## Zurechtschneiden von Entscheidungsbäumen\n",
    "\n",
    "Eine effektive Strategie zur Bekämpfung des Overfittings bei Entscheidungsbäumen\n",
    "ist das sogenannte **Pruning**, also das Beschneiden des Baumes. Pruning hilft,\n",
    "die Komplexität des Modells zu reduzieren, indem weniger relevante\n",
    "Entscheidungszweige nach bestimmten Kriterien entfernt werden. Im Kontext\n",
    "unseres Autohaus-Beispiels würde dies bedeuten, dass Entscheidungszweige, die\n",
    "beispielsweise aufgrund von Ausreißern entstanden sind, abgeschnitten werden.\n",
    "Dies könnte beispielsweise den zuvor erwähnten dünnen blauen Streifen bei einem\n",
    "Kilometerstand von ungefähr 97000 km betreffen, der wahrscheinlich durch einen\n",
    "Ausreißer entstanden ist. Durch das Entfernen solcher spezifischen Anpassungen\n",
    "kann der Entscheidungsbaum besser verallgemeinern und wird robuster gegenüber\n",
    "neuen, unbekannten Daten. Das Ergebnis ist ein Modell, das eine bessere Balance\n",
    "zwischen Anpassung an die Trainingsdaten und Generalisierungsfähigkeit aufweist.\n",
    "\n",
    "Für Entscheidungsbäume gibt es prinzipiell zwei Methoden des Prunings:\n",
    "**Prä-Pruning** und **Post-Pruning**. Das Prä-Pruning findet *vor* dem Training\n",
    "des Entscheidungsbaumes statt, das Post-Pruning *nach* dem Training. Die beiden\n",
    "wichtigsten Prä-Pruning-Maßnahmen sind\n",
    "\n",
    "* die Begrenzung der maximalen Tiefe des Baumes und\n",
    "* die Forderung nach einer Mindestanzahl von Datenpunkten (entweder pro Knoten\n",
    "  oder pro Blatt).\n",
    "\n",
    "Beim Post-Pruning werden im Nachhinein Knoten mit wenig Informationen aus dem\n",
    "Entscheidungsbaum entfernt oder es werden Knoten zusammengelegt. Scikit-Learn\n",
    "hat nur Prä-Pruning implementiert, so dass wir hier nicht weiter auf\n",
    "Post-Pruning eingehen.\n",
    "\n",
    "### Prä-Pruning: Baumtiefe\n",
    "\n",
    "Wir schauen uns zunächst an, wie bei Scikit-Learn-Entscheidungsbäumen die\n",
    "maximale Tiefe festgelegt wird. Bisher haben wir das Modell ohne weitere\n",
    "Parameter initialisiert (einzige Ausnahme: wir haben ggf. den\n",
    "Zufallszahlengenerator aus didaktischen Gründen fixiert, damit die Ergebnisse\n",
    "vergleichbar sind). Nun verwenden wir bei der Initialisierung des\n",
    "DecisionTreeClassifiers das optionale Argument `max_depth=` und setzen es auf\n",
    "`1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell_tiefe1 = DecisionTreeClassifier(random_state=0, max_depth=1)\n",
    "modell_tiefe1.fit(X,y)\n",
    "\n",
    "plot_tree(modell_tiefe1,\n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9fbb0",
   "metadata": {},
   "source": [
    "Eine Tiefe von 1 bedeutet, dass nur noch eine einzige Entscheidungsfrage\n",
    "gestellt wird. Das reicht nicht mehr, um die Autos in reine Blätter zu\n",
    "sortieren. Im linken Blatt sind 13 nicht verkaufte Autos und 24 verkaufte Autos,\n",
    "weshalb diesem Blatt die Kategorie »verkauft« zugeordnet wird. Im rechten Blatt\n",
    "sind 12 nicht verkaufte Autos und ein verkauftes Auto, so dass dieses Blatt\n",
    "insgesamt als »nicht verkauft« gilt. Die Visualisierung der Entscheidungsgrenzen\n",
    "zeigt, um welche Autos es sich handelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e56c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = DecisionBoundaryDisplay.from_estimator(modell_tiefe1, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\n",
    "fig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\n",
    "fig.ax_.set_title('Entscheidungsgrenzen');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5e21a",
   "metadata": {},
   "source": [
    "Insbesondere die Visualisierung der Entscheidungsgrenzen zeigt aber auch, dass\n",
    "dieser Entscheidungsbaum nicht besonders gut die Daten erklärt. Der Score ist\n",
    "mit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147ae218",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Score des Entscheidungsbaumes mit Tiefe 1: {modell_tiefe1.score(X,y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9ab0a",
   "metadata": {},
   "source": [
    "auch nicht so gut. Daher verwenden wir nun als maximale Tiefe des Entscheidungsbaumes einen Wert von 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbfe418",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell_tiefe2 = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "modell_tiefe2.fit(X,y)\n",
    "\n",
    "plot_tree(modell_tiefe2,\n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);\n",
    "\n",
    "print(f'Score des Entscheidungsbaumes mit Tiefe 2: {modell_tiefe2.score(X,y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c51a77",
   "metadata": {},
   "source": [
    "Mit einem Score von 0.78 ist der Entscheidungsbaum mit einer maximalen Tiefe von\n",
    "2 zwar besser als der Baum mit einer maximalen Tiefe von 1, aber deutlich\n",
    "entfernt von dem Score 1.0 bei einer Baumtiefe von 7. Die Entscheidungsgrenzen\n",
    "sehen folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35177a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = DecisionBoundaryDisplay.from_estimator(modell_tiefe2, X, cmap=ListedColormap(['#EF553B33', '#636EFA33']), grid_resolution=1000)\n",
    "fig.ax_.scatter(X['Kilometerstand [km]'], X['Preis [EUR]'], c=y, cmap=ListedColormap(['#EF553B', '#636EFA']))\n",
    "fig.ax_.set_title('Entscheidungsgrenzen');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d50274",
   "metadata": {},
   "source": [
    "Was ist jetzt besser, eine maximale Tiefe von 1 oder 2? Oder doch 3 vielleicht?\n",
    "Die Einführung der maximalen Tiefe bietet den Vorteil, das Overfitting zu\n",
    "bekämpfen. Der Nachteil davon ist, dass wir jetzt einen neuen Parameter haben,\n",
    "der das Training und die Prognose des Modells bestimmt. Und für diesen Parameter\n",
    "muss ein passender Wert eingestellt werden. Solche Parameter nennt man\n",
    "**Hyperparameter**.\n",
    "\n",
    "Ein Hyperparameter ist ein Parameter, der vor dem Training eines Modells\n",
    "festgelegt wird und nicht aus den Daten während des Trainings gelernt wird. Die\n",
    "Hyperparameter steuern den gesamten Lernprozess und haben einen wesentlichen\n",
    "Einfluss auf die Leistung des Modells.\n",
    "\n",
    "Ein Score von 1.0 auf den Trainingsdaten deutet auf Overfitting hin, d.h. das\n",
    "Modell hat die Daten auswendig gelernt. Ein sehr niedriger Score (z.B. 0.72)\n",
    "deutet auf Underfitting hin, d.h. das Modell ist zu einfach. Das Ziel ist ein\n",
    "Gleichgewicht: ein Score, der hoch genug ist, um die Daten gut zu beschreiben,\n",
    "aber nicht 1.0, um Generalisierung zu ermöglichen. Werte zwischen 0.8 und 0.95\n",
    "sind oft ein guter Kompromiss, aber dies muss mit separaten Testdaten validiert\n",
    "werden.\n",
    "\n",
    "Kommen wir nun zu einem anderen Hyperparameter der Entscheidungsbäume, der\n",
    "Mindestanzahl von Datenpunkten.\n",
    "\n",
    "### Prä-Pruning: Mindestanzahl Datenpunkte\n",
    "\n",
    "Genau wie der Hyperparameter zur Begrenzung der Baumtiefe wird die Mindestanzahl\n",
    "der Datenpunkte vorab bei der Initialisierung des Entscheidungsbaumes\n",
    "festgelegt. Scikit-Learn bietet wiederum zwei Möglichkeiten, über die minimale\n",
    "Anzahl von Datenpunkten den Entscheidungsbaum zurechtzuschneiden. Zum einen kann\n",
    "für die *Knoten* eine minimal erforderliche Anzahl von Datenpunkten festgelegt\n",
    "werden, ab der es erlaubt ist, durch Entscheidungsfragen weiter zu verzweigen.\n",
    "Zum anderen kann eine minimale Anzahl an Datenpunkten für jedes *Blatt*\n",
    "festgelegt werden, das am Ende der Verzweigungen erreicht werden muss.\n",
    "\n",
    "Wir probieren beide Möglichkeiten aus und vergleichen die Ergebnisse\n",
    "miteinander. Die Option zur Einstellung der Mindestanzahl pro Knoten heißt\n",
    "`min_samples_split` und die Option zur Einstellung des Mindestanzahl Datenpunkte\n",
    "pro Blatt heißt `min_samples_leaf`. Beiden optionalen Argumenten kann entweder\n",
    "ein Integer übergeben werden oder ein Float. Wird ein Integer übergeben, so ist\n",
    "damit die tatsächliche minimale Anzahl an Datenpunkten gemeint. Ein Float wird\n",
    "als Bruch interpretiert und meint die relative Anzahl der Datenpunkte. Der Bruch\n",
    "wird mit der Gesamtzahl der Datenpunkte multipliziert und dann wird auf die\n",
    "nächste ganze Zahl aufgerundet.\n",
    "\n",
    "Schauen wir uns beide Varianten an. Zunächst begrenzen wir die Knoten und\n",
    "fordern, dass sich in jedem Entscheidungsknoten mindestens sechs Datenpunkte\n",
    "befinden müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c916b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell_knotenbegrenzung = DecisionTreeClassifier(random_state=0, min_samples_split=6)\n",
    "modell_knotenbegrenzung.fit(X,y)\n",
    "\n",
    "plot_tree(modell_knotenbegrenzung,\n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);\n",
    "\n",
    "print(f'Score des Entscheidungsbaumes mit Prä-Pruning Mindestanzahl Datenpunkte pro Knoten: {modell_knotenbegrenzung.score(X,y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88168a90",
   "metadata": {},
   "source": [
    "Der Score ist 0.92. Nun fordern wir, dass in jedem Blatt mindestens sechs\n",
    "Datenpunkte verbleiben müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell_blattbegrenzung = DecisionTreeClassifier(random_state=0, min_samples_leaf=6)\n",
    "modell_blattbegrenzung.fit(X,y)\n",
    "\n",
    "plot_tree(modell_blattbegrenzung,\n",
    "    feature_names=['Kilometerstand [km]', 'Preis [EUR]'],\n",
    "    class_names=['nicht verkauft', 'verkauft']);\n",
    "\n",
    "print(f'Score des Entscheidungsbaumes mit Prä-Pruning Mindestanzahl Datenpunkte pro Blatt: {modell_blattbegrenzung.score(X,y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e47eb4",
   "metadata": {},
   "source": [
    "In diesem Fall erhalten wir einen Entscheidungsbaum mit einem Score von 0.82.\n",
    "Was jetzt die bessere Wahl ist -- Begrenzung der Baumtiefe oder Festlegung einer\n",
    "Mindestanzahl von Datenpunkten Knoten/Blatt -- und vor allem welchen Wert der\n",
    "Hyperparameter haben soll, ist eine zentrale Herausforderung im maschinellen\n",
    "Lernen. In späteren Kapiteln werden wir systematische Methoden wie Grid Search\n",
    "und Cross-Validation kennenlernen, um die besten Hyperparameter-Werte zu finden.\n",
    "\n",
    "**Mini-Übung**\n",
    "Welcher Entscheidungsbaum zeigt vermutlich die stärkste Tendenz zum Overfitting?\n",
    "Stellen Sie eine Vermuting an und überprüfen Sie Ihre Vermutung durch Ausprobieren.\n",
    "\n",
    "A) `DecisionTreeClassifier(max_depth=2)`  \n",
    "B) `DecisionTreeClassifier(max_depth=10)`  \n",
    "C) `DecisionTreeClassifier(min_samples_leaf=20)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier Ihr Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce09b1c",
   "metadata": {},
   "source": [
    "## Zusammenfassung und Ausblick\n",
    "\n",
    "In diesem Kapitel haben wir die Tendenz der Entscheidungsbäume zum Overfitting\n",
    "diskutiert. Um dem Problem des Overfittings zu begegnen, bietet Scikit-Learn die\n",
    "Möglichkeit des Prä-Prunings. Durch die Begrenzung der maximalen Baumtiefe oder\n",
    "die Festlegung einer Mindestanzahl von Datenpunkten in Knoten oder Blättern kann\n",
    "Overfitting reduziert werden. Diese zusätzlichen Parameter des\n",
    "Entscheidungsbaums werden Hyperparameter genannt und müssen angepasst werden.\n",
    "Eine weitere Alternative, das Overfitting von Entscheidungsbäumen zu minimieren,\n",
    "bieten die Random Forests, die wir in einem späteren Kapitel kennenlernen\n",
    "werden."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
