{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de3b6d2",
   "metadata": {},
   "source": [
    "# 12.2 Mehrschichtiges Perzeptron\n",
    "\n",
    "## Lernziele\n",
    "\n",
    "\n",
    "* Sie können das Konzept eines **künstlichen Neurons** erklären.\n",
    "* Sie wissen, was ein **mehrschichtiges Perzeptron** ist und kennen den\n",
    "  englischen Begriff **Multilayer Perceptron** (MLP) dafür.\n",
    "* Sie können den Begriff **Deep Learning** erklären.\n",
    "\n",
    "\n",
    "+++\n",
    "\n",
    "## Künstliche Neuronen\n",
    "\n",
    "Im letzten Kapitel haben wir das Perzeptron kennengelernt. Schematisch können\n",
    "wir es folgendermaßen darstellen:\n",
    "\n",
    "![Schematische Darstellung eines Perzeptrons](pics/fig_12_02_topology_perceptron.svg)\n",
    "\n",
    "Kurz zusammengefasst funktioniert ein Perzeptron ähnlich wie ein biologisches\n",
    "Neuron. Jedes Eingangssignal wird mit einem Gewicht multipliziert. Anschließend\n",
    "werden die gewichteten Eingangssignale summiert. Übersteigt die gewichtete Summe\n",
    "einen Schwellenwert, feuert sozusagen das Neuron. Das Ausgabesignal wird\n",
    "aktiviert.\n",
    "\n",
    "Mathematisch gesehen, werden auf die Eingabedaten zwei mathematische Funktionen\n",
    "angewendet:\n",
    "\n",
    "1. Übertragungsfunktion (hier die Bildung der gewichteten Summe) und\n",
    "2. Aktivierungsfunktion (hier die Anwendung der Heaviside-Funktion).\n",
    "\n",
    "Dieses Prinzip behalten wir bei, erlauben aber eine größere Auswahl an\n",
    "Übertragungs- und Aktivierungsfunktionen, um das Perzeptron auf das sogenannte\n",
    "**künstliche Neuron** zu verallgemeinern. Später werden wir die künstlichen\n",
    "Neuronen in einem Netz zusammensetzen.\n",
    "\n",
    "**Was ist ... ein künstliches Neuron?**\n",
    "\n",
    "Ein künstliches Neuron verarbeitet Informationen, indem es\n",
    "\n",
    "1. Eingaben mit Gewichten multipliziert und zusammenaddiert,\n",
    "2. eine Aktivierungsfunktion auf die gewichtete Summe anwendet und\n",
    "3. das Ergebnis ausgibt.\n",
    "\n",
    "Bei neuronalen Netzen werden vor allem die\n",
    "[ReLU-Funktion](https://de.wikipedia.org/wiki/Rectifier_(neuronale_Netzwerke))\n",
    "(rectified linear unit) und der [Tangens\n",
    "hyperbolicus](https://de.wikipedia.org/wiki/Tangens_hyperbolicus_und_Kotangens_hyperbolicus)\n",
    "als Aktivierungsfunktion verwendet, die im Folgenden dargestellt werden.\n",
    "\n",
    "ReLU-Funktion:\n",
    "\n",
    "![ReLU-Funktion](pics/fig_12_02_plot_relu_function.svg)\n",
    "\n",
    "Tangens hyperbolicus:\n",
    "\n",
    "![Tangens hyperbolicus](pics/fig_12_02_plot_tanh_function.svg)\n",
    "\n",
    "Beide Funktionen haben einen glatten Verlauf (keinen harten Sprung wie die\n",
    "Heaviside-Funktion). Daher können die Gewichte $w_0, w_1, \\ldots, w_n$ mit\n",
    "mathematischen Optimierungsverfahren besser bestimmt werden als beim klassischen\n",
    "Perzeptron mit der Heaviside-Funktion. Im nächsten Kapitel werden wir sehen, wie\n",
    "das in der Praxis mit Scikit-Learn funktioniert. Vorher beschäftigen wir uns mit\n",
    "dem Zusammensetzen von vielen Perzeptronen zu einem neuronalen Netz.\n",
    "\n",
    "## Aus Neuronen wird ein Netz\n",
    "\n",
    "Neuronale Netze sind Netze aus einzelnen künstlichen Neuronen. Im Folgenden\n",
    "werden wir auf eine mathematisch präzise Beschreibung von neuronalen Netzen\n",
    "verzichten und stattdessen das Konzept eines neuronalen Netzes anhand von\n",
    "Schemazeichnungen erläutern. Dazu vereinfachen wir zunächst die schematische\n",
    "Darstellung des künstlichen Neurons. Zunächst fassen wir die beiden\n",
    "mathematischen Operationen »Übertragungsfunktion« (Bilden der gewichteten Summe)\n",
    "und »Aktivierungsfunktion« in einem gemeinsamen Kreis in der Mitte zusammen. Die\n",
    "Bias-Einheit lassen wir in der Darstellung ebenfalls weg.\n",
    "\n",
    "![Vereinfachte schematische Darstellung eines künstlichen Neurons](pics/fig_12_02_neuron_with_annotations.svg)\n",
    "\n",
    "Tatsächlich sind sogar häufig Darstellungen verbreitet, bei denen die\n",
    "Beschriftungen komplett weggelassen werden.\n",
    "\n",
    "![Symbolbild eines künstlichen Neurons](pics/fig_12_02_neuron_without_annotations.svg)\n",
    "\n",
    "Mit Hilfe dieser stark vereinfachten Darstellung eines künstlichen Neurons\n",
    "zeigen wir nun, wie aus vielen solcher künstlicher Neuronen ein neuronales Netz\n",
    "zusammengesetzt wird. In einem ersten Schritt werden die Eingaben für jedes\n",
    "Merkmal (in diesem Beispiel vier Merkmale symbolisiert durch hellblaue Kreise)\n",
    "gewichtet, aufsummiert und dann wird darauf eine Aktivierungsfunktion angewendet\n",
    "(weißer Kreis oben). Diesen Vorgang wiederholen wir mit anderen Gewichten oder\n",
    "einer anderen Aktivierungsfunktion, also mit einem zweiten künstlichen Neuron\n",
    "(weißer Kreis unten). In einem zweiten Schritt werden die beiden Ergebnisse der\n",
    "einzelnen künstlichen Neuronen wiederum verrechnet, um daraus die finale Ausgabe\n",
    "zu prognostizieren (dunkelblauer Kreis). Die folgende Schemazeichnung\n",
    "verdeutlicht diese Vorgehensweise für vier Merkmale (die Bias-Einheit wurde\n",
    "wieder weggelassen).\n",
    "\n",
    "![Ein mehrschichtiges Perzeptron (Multilayer Perceptron)](pics/fig_12_02_multi_layer_perceptron.svg)\n",
    "\n",
    "Bei dieser Architektur unterscheiden wir drei Arten von Schichten.\n",
    "\n",
    "1. Die **Eingabeschicht** (auf Englisch: Input Layer) nimmt die Eingabedaten\n",
    "   entgegen. Jedes Neuron in der Eingabeschicht entspricht einem Merkmal oder\n",
    "   einer Eigenschaft der Eingabedaten.\n",
    "2. Die **Ausgabeschicht** (auf Englisch: Output Layer) liefert das Ergebnis der\n",
    "   Berechnung. Bei einer binären Klassifikationsaufgabe besteht die\n",
    "   Ausgabeschicht beispielsweise aus einem einzigen Neuron.\n",
    "3. Zwischen Eingabe- und Ausgabeschicht liegen die **versteckten Schichten**\n",
    "   (auf Englisch: Hidden Layers). Diese Schichten heißen versteckt, weil wir von\n",
    "   außen nur die Eingabe und die Ausgabe sehen, nicht aber die\n",
    "   Zwischenergebnisse in diesen Schichten. Die versteckten Schichten ermöglichen\n",
    "   es dem neuronalen Netz, komplexe nichtlineare Zusammenhänge zwischen Eingabe\n",
    "   und Ausgabe zu erlernen. Je mehr versteckte Schichten ein neuronales Netz\n",
    "   hat, desto komplexere Muster kann es prinzipiell erfassen.\n",
    "\n",
    "Ab etwa 2-3 versteckten Schichten spricht man von **Deep Learning**.\n",
    "\n",
    "**Mini-Übung**\n",
    "\n",
    "Betrachten Sie das mehrschichtige Perzeptron aus der Abbildung oben. Es hat vier\n",
    "Eingabeneuronen, eine versteckte Schicht mit zwei Neuronen und ein\n",
    "Ausgabeneuron.\n",
    "\n",
    "1. Wie viele Gewichte verbinden die Eingabeschicht mit der versteckten Schicht?\n",
    "2. Wie viele Gewichte verbinden die versteckte Schicht mit der Ausgabeschicht?\n",
    "3. Wie viele Bias-Einheiten benötigt das Netz insgesamt?\n",
    "4. Berechnen Sie die Gesamtzahl aller Parameter (Gewichte + Bias-Einheiten) des\n",
    "   Netzes. Mit 'Parameter' meinen wir alle Zahlen, die das Netz lernen muss,\n",
    "   also alle Gewichte und Bias-Einheiten zusammen.\n",
    "\n",
    "## Zusammenfassung und Ausblick\n",
    "\n",
    "Nachdem wir in diesem Kapitel das Konzept eines neuronalen Netzes erklärt haben,\n",
    "werden wir im nächsten Kapitel ein neuronales Netz mit Scikit-Learn trainieren.\n",
    "Dabei werden wir lernen, wie das Netz die optimalen Gewichte automatisch findet."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb",
   "main_language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
